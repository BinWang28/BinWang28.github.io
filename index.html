<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
<!-- MathJax -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async>
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
</head>
<body>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-GQSE2H4QXE"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-GQSE2H4QXE');
</script>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Bin. Wang</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="activities.html">Activities</a></div>
<div class="menu-item"><a href="biography.html">Biography</a></div>
<div class="menu-item"><a href="data/Resume_Bin.pdf">CV</a></div>
<div class="menu-item"><a href="misc.html">Misc.</a></div>
<div class="menu-category">Research</div>
<div class="menu-item"><a href="team.html">Team</a></div>
<div class="menu-item"><a href="publications.html">Publication</a></div>
<div class="menu-category">Teaching</div>
<div class="menu-item"><a href="teaching.html">Courses</a></div>
<div class="menu-category">Website</div>
<div class="menu-item"><a href="index_chinese.html">中文</a></div>
<div class="menu-item"><a href="statistics.html">Statistics</a></div>
</td>
<td id="layout-content">
<h2>Bin Wang</h2>
<table class="imgtable"><tr><td>
<a href="https://binwang.xyz/"><img src="data/profile.png" alt="alt text" width="210px" height="210px" /></a>&nbsp;</td>
<td align="left"><p>Scientist
<br />
Institute for Infocomm Research <a href="https://www.a-star.edu.sg/i2r" target=&ldquo;blank&rdquo;>(I\(^2\)R)</a>, <a href="https://www.a-star.edu.sg" target=&ldquo;blank&rdquo;>A*STAR</a>, Singapore
<br />
Address: 1 Fusionopolis Way, Level 12, Connexis South Tower, Singapore 138632
<br />
Personal Email: <i>bwang28c</i> [@] gmail.com 
<br />
Work Email: wang_bin / [@] i2r.a-star.edu.sg
<br />
[<a href="https://scholar.google.com/citations?user=jUrRMv4AAAAJ&amp;hl=en" target=&ldquo;blank&rdquo;>Google Scholar</a>]&nbsp;&nbsp;&nbsp;
[<a href="https://github.com/BinWang28" target=&ldquo;blank&rdquo;>GitHub</a>]&nbsp;&nbsp;&nbsp;
[<a href="https://huggingface.co/binwang" target=&ldquo;blank&rdquo;>HuggingFace</a>]&nbsp;&nbsp;&nbsp;
[<a href="https://www.linkedin.com/in/bin-wang-3b7054140/" target=&ldquo;blank&rdquo;>LinkedIn</a>]&nbsp;&nbsp;&nbsp;
[<a href="https://twitter.com/BinWang_Eng" target=&ldquo;blank&rdquo;>Twitter</a>]
</p>
</td></tr></table>
<h2>News</h2>
<ul>
<li><p>2025.01&nbsp;Our AudioBench work is officially accepted to NAACL 2025 Main Conference! <a href="https://huggingface.co/spaces/MERaLiON/AudioBench-Leaderboard" target=&ldquo;blank&rdquo;>Leaderboard</a> and <a href="https://github.com/AudioLLMs/AudioBench" target=&ldquo;blank&rdquo;>Code</a>.
</p>
</li>
<li><p>2024.12&nbsp;We released <a href="https://huggingface.co/MERaLiON" target=&ldquo;blank&rdquo;>MERaLiON models</a>, the first audio-based large language model designed specifically for Singlish and other related tasks!
</p>
</li>
<li><p>2024-2026&nbsp;Our team is working on National LLM Project <a href="https://www.imda.gov.sg/resources/press-releases-factsheets-and-speeches/press-releases/2023/sg-to-develop-southeast-asias-first-llm-ecosystem" target=&ldquo;blank&rdquo;>Press Coverage</a>.
</p>
</li>
<li><p>2024&nbsp;AudioBench is released with leaderboard and evaluation toolkit - <a href="https://arxiv.org/abs/2406.16020" target=&ldquo;blank&rdquo;>AudioLLM Evaluation</a>.
</p>
</li>
<li><p>2024&nbsp;NAACL <a href="https://seaeval.github.io/" target=&ldquo;blank&rdquo;>SeaEval for Multilingual Evaluation</a>.
</p>
</li>
</ul>
<h2>About Me</h2>
<p>I am a scientist at Aural & Language Intelligence Department, I2R, A*STAR. Before joing that, I was a research fellow at National University of Singapore (<a href="https://www.nus.edu.sg/" target=&ldquo;blank&rdquo;>NUS</a>) working with Prof. <a href="https://colips.org/~eleliha/" target=&ldquo;blank&rdquo;>Haizhou Li</a> from 2021-2023. I received my Ph.D. degree from University of Southern California (<a href="https://www.usc.edu/" target=&ldquo;blank&rdquo;>USC</a>) supervised by Prof. <a href="https://viterbi.usc.edu/directory/faculty/Kuo/Chung-Chieh" target=&ldquo;blank&rdquo;>C.-C. Jay Kuo</a> in 2021. My bachelor's degree is obtained from University of Electronic Science and Technology of China (<a href="https://en.uestc.edu.cn/" target=&ldquo;blank&rdquo;>UESTC</a>) in 2017.
</p>
<h3>Some of the topics that I am currently researching include:</h3>
<ol>
<li><p><b>Make LLM can hear</b> - AudioLLM - Audio-Based Large Language Models
</p>
<ol>
<li><p>What techniques can be used to effectively integrate audio processing capabilities into existing LLM architectures? 
</p>
</li>
<li><p>What is the most efficient approach for achieving seamless cross-modality integration? 
</p>
</li>
<li><p>What benchmarks can be designed to accurately evaluate the real-world performance of AudioLLMs?
</p>
</li>
<li><p>Current Outcomes: <a href="https://huggingface.co/MERaLiON" target=&ldquo;blank&rdquo;>MERaLiON-AduioLLM</a>, <a href="https://github.com/AudioLLMs/AudioBench" target=&ldquo;blank&rdquo;>AudioBench</a>, <a href="https://github.com/AudioLLMs/Awesome-Audio-LLM" target=&ldquo;blank&rdquo;>Awesome-Audio-LLM</a>, <a href="https://arxiv.org/abs/2409.06635" target=&ldquo;blank&rdquo;>MoWE-Audio</a>
</p>
</li></ol>
</li>
<li><p><b>Multilingal and Multicultual LLM</b>
</p>
<ol>
<li><p>What unique properties should a multilingual LLM possess to cater to diverse languages effectively?
</p>
</li>
<li><p>How can multilingual learning be made more efficient and effective, especially for low-resource languages?
</p>
</li>
<li><p>What internal mechanisms can ensure robust multilingual knowledge alignment within the model?
</p>
</li>
<li><p>Current Outcomes: <a href="https://aclanthology.org/2024.naacl-long.22/" target=&ldquo;blank&rdquo;>SeaEval</a>, <a href="https://aclanthology.org/2024.c3nlp-1.4/" target=&ldquo;blank&rdquo;>CRAFT</a>, <a href="https://arxiv.org/abs/2404.11932" target=&ldquo;blank&rdquo;>CrossIn</a>, <a href="https://arxiv.org/abs/2406.10118" target=&ldquo;blank&rdquo;>SEACrowd</a>
</p>
</li></ol>
</li>
<li><p><b>Conversional AI</b>
</p>
<ol>
<li><p>Representation Learning for Retrieval-Augmented Generation, Knowledge Graphs
</p>
</li>
<li><p>What representation and coordination strategies can enhance multi-agent communication in shared environments?
</p>
</li>
<li><p>What methods can enable conversational agents to effectively reason and plan based on learned or provided world models?
</p>
</li>
<li><p>Current Outcomes: <a href="https://dl.acm.org/doi/abs/10.1109/TASLP.2020.3008390" target=&ldquo;blank&rdquo;>Representation Learning</a>, <a href="https://ieeexplore.ieee.org/document/9534355" target=&ldquo;blank&rdquo;>Commonsense Knowledge Graph</a>    
</p>
</li>
</ol>

</li>
</ol>
<h2>Opportunities</h2>
<div class="infoblock">
<div class="blockcontent">
<p>We are actively looking for candidates working on Multimodal LLMs (text, audio, vision, etc.).
<br />
</p>
<ul>
<li><p>Research Interns (6 months or above preferred), 
<br />
</p>
<ul>
<li><p><a href="https://www.a-star.edu.sg/Scholarships/for-undergraduate-studies/singapore-international-pre-graduate-award-sipga" target=&ldquo;blank&rdquo;>SIPGA</a> for international (master / undergraduate) students.
</p>
</li>
<li><p><a href="https://www.a-star.edu.sg/Scholarships/for-undergraduate-studies/a-star-research-internship-award-aria" target=&ldquo;blank&rdquo;>ARIA</a> for Singaporean undergraduate students.
</p>
</li>
<li><p><a href="https://www.a-star.edu.sg/Scholarships/for-graduate-studies/a-star-research-attachment-programme" target=&ldquo;blank&rdquo;>ARAP</a> for international Ph.D. students attachment for 1-2 years.
</p>
</li>
<li><p>Local students from NUS, NTU, SMU, SUTD, SIT, Polytechnic etc. please contact me directly for attachment to projects.
<br />
</p>
</li></ul>
</li>
<li><p>Ph.D. Students
<br />
</p>
<ul>
<li><p><a href="https://www.a-star.edu.sg/Scholarships/for-graduate-studies" target=&ldquo;blank&rdquo;>SIPGA, AGS, ACIS</a>
<br />
</p>
</li></ul>
</li>
<li><p>Long-term Positions
</p>
<ul>
<li><p>Research Engineer / Scientist (Both engineering and research background are preferred.)
</p>
</li>
</ul>

</li>
</ul>
</div></div>
<h2>Some Publications </h2>
<ol>
<li><p>Bin Wang, Xunlong Zou, Geyu Lin, Shuo Sun, Zhuohan Liu, Wenyu Zhang, Zhengyuan Liu, AiTi Aw, Nancy F. Chen. &ldquo;AudioBench: A Universal Benchmark for Audio Large Language Models.&rdquo; NAACL, 2025. [<a href="https://arxiv.org/abs/2406.16020" target=&ldquo;blank&rdquo;>paper</a>], [<a href="https://github.com/AudioLLMs/AudioBench" target=&ldquo;blank&rdquo;>code</a>]
</p>
</li>
<li><p>Bin Wang, Zhengyuan Liu, Xin Huang, Fangkai Jiao, Yang Ding, AiTi Aw, Nancy F. Chen. &ldquo;SeaEval for Multilingual Foundation Models: From Cross-Lingual Alignment to Cultural Reasoning.&rdquo; NAACL, 2024. [<a href="https://aclanthology.org/2024.naacl-long.22/" target=&ldquo;blank&rdquo;>paper</a>], [<a href="https://github.com/SeaEval/SeaEval" target=&ldquo;blank&rdquo;>code</a>]
</p>
</li>
<li><p>Bin Wang, Chen Zhang, Yan Zhang, Yiming Chen and Haizhou Li. &ldquo;Analyzing and Evaluating Faithfulness in Dialogue Summarization.&rdquo; EMNLP, 2022. [<a href="https://arxiv.org/abs/2210.11777" target=&ldquo;blank&rdquo;>paper</a>], [<a href="https://github.com/BinWang28/FacEval" target=&ldquo;blank&rdquo;>code</a>]
</p>
</li>
<li><p>Bin Wang, C.-C. Jay Kuo, and Haizhou Li. &ldquo;Just Rank: Rethinking Evaluation with Word and Sentence Similarities.&rdquo; ACL, 2022. [<a href="https://arxiv.org/abs/2203.02679" target=&ldquo;blank&rdquo;>paper</a>], [<a href="https://github.com/BinWang28/EvalRank-Embedding-Evaluation" target=&ldquo;blank&rdquo;>code</a>]
</p>
</li>
<li><p>Bin Wang, Guangtao Wang, Jing Huang, Jiaxuan You, Jure Leskovec, and C.-C. Jay Kuo. &ldquo;Inductive learning on commonsense knowledge graph completion.&rdquo; IJCNN, 2021. [<a href="https://arxiv.org/abs/2009.09263" target=&ldquo;blank&rdquo;>paper</a>], [<a href="https://github.com/BinWang28/InductivE" target=&ldquo;blank&rdquo;>code</a>]
</p>
</li>
<li><p>Bin Wang, and C.-C. Jay Kuo. &ldquo;SBERT-WK: A sentence embedding method by dissecting bert-based word models.&rdquo; IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2020. [<a href="https://ieeexplore.ieee.org/document/9140343" target=&ldquo;blank&rdquo;>paper</a>], [<a href="https://github.com/BinWang28/SBERT-WK-Sentence-Embedding" target=&ldquo;blank&rdquo;>code</a>]
</p>
</li>
<li><p>Bin Wang*, Angela Wang*, Fenxiao Chen, Yuncheng Wang, and C.-C. Jay Kuo. &ldquo;Evaluating word embedding models: methods and experimental results.&rdquo; APSIPA transactions on signal and information processing, 2019. [<a href="https://www.cambridge.org/core/journals/apsipa-transactions-on-signal-and-information-processing/article/evaluating-word-embedding-models-methods-and-experimental-results/EDF43F837150B94E71DBB36B28B85E79" target=&ldquo;blank&rdquo;>paper</a>], [<a href="https://github.com/BinWang28/Word-Embedding-Eval" target=&ldquo;blank&rdquo;>code</a>]
</p>
</li>
</ol>
<p><a href="publications.html" target=&ldquo;blank&rdquo;>Full list of publications</a>.
</p>
<div id="footer">
<div id="footer-text">
Page generated 2025-02-13 15:33:34 CST, by jemdoc+MathJax.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
